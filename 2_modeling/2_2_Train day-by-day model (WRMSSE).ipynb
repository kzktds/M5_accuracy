{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "from multiprocessing import Pool\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各種パラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5678                      # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913+28            # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "\n",
    "### ファイルのパスは適宜変更してください ### \n",
    "\n",
    "# 読み込みが必要なpklファイルのパス\n",
    "LAGS     = './lags_df_1_dbd.pkl'\n",
    "\n",
    "# 読み込みが必要なcsvファイルのパス\n",
    "EVALUATION   = './sales_train_evaluation.csv'\n",
    "CALENDAR_CSV = './calendar.csv'\n",
    "PRICE_CSV    = './sell_prices.csv'\n",
    "SAMPLE_CSV   = './sample_submission.csv'\n",
    "\n",
    "# WRMSSE計算用pklへのパス\n",
    "SW_DF    = './sw_df_update.pkl'\n",
    "ROLL_MAT = './roll_mat_df_update.pkl'\n",
    "\n",
    "# 店舗別マスタが格納されているフォルダへのパス\n",
    "GRID_PATH = \"./\"\n",
    "\n",
    "# CVを行う区切りのリスト\n",
    "# SPLIT_LIST = [[1858, 1885], [1886, 1913], [1914, 1941]]\n",
    "SPLIT_LIST = [[1942, 1969]]\n",
    "\n",
    "iteration_list = pd.read_pickle(\"./rounds_dict_wrmsse.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'poisson',\n",
    "                'tweedie_variance_power': 1.1,\n",
    "                'metric': 'rmse',\n",
    "                'subsample': 0.5,\n",
    "                'subsample_freq': 1,\n",
    "                'learning_rate': 0.03,\n",
    "                'num_leaves': 2**11-1,\n",
    "                'min_data_in_leaf': 2**12-1,\n",
    "                'feature_fraction': 0.5,\n",
    "                'max_bin': 100,\n",
    "                'n_estimators': 5000,\n",
    "                'boost_from_average': False,\n",
    "                'verbose': -1,\n",
    "            } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feat = [\n",
    "    \"event_name_1\", \"tm_wm\", \"tm_dw\", \"tm_w_end\", \"tm_y\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(original_df, feat_path, feat_names=None, merge_key=None, day=0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    パスとキーを指定して、特徴量を追加する   \n",
    "    - original_df : 特徴を追加するDataFrame\n",
    "    - feat_path   : 追加する特徴(pkl)のパス\n",
    "    - feat_names  : pklの中で、結合する特徴を絞り込みたい場合に指定する\n",
    "    - merge_key   : pklをmergeで結合する場合、キーを指定する(Noneの場合concatする)\n",
    "    - day         : ラグ変数を結合する際にずらす日数(1日後予測モデルはday=1を引数として与える)\n",
    "    \n",
    "    \"\"\"\n",
    "    tmp = pd.read_pickle(feat_path)\n",
    "    \n",
    "    if day > 1:\n",
    "        tmp[\"d\"]= tmp[\"d\"] + (day-1)\n",
    "    \n",
    "    if feat_names is not None: \n",
    "        tmp = tmp[feat_names]\n",
    "        \n",
    "    if merge_key is None:\n",
    "        tmp = tmp[tmp.index.isin(original_df.index)]\n",
    "        original_df = pd.concat([original_df, tmp], axis=1)\n",
    "    else:\n",
    "        original_df = pd.merge(original_df, tmp, on=merge_key, how=\"left\")\n",
    "    \n",
    "    return original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_store(store, day=0):\n",
    "    \"\"\"\n",
    "    store_idを指定して、学習用のデータフレームを取得する\n",
    "    \"\"\"  \n",
    "    # 特徴結合済みの店舗別データフレームを読み込む\n",
    "    pickle_name = GRID_PATH + \"grid_df_master_\"+store+\".pkl\"\n",
    "    df = pd.read_pickle(pickle_name)\n",
    "    \n",
    "    # ラグ特徴(LAGS)を結合する\n",
    "    lag = pd.read_pickle(LAGS)\n",
    "    lag_features = list(lag.columns[3:])\n",
    "    df = merge_df(df, feat_path=LAGS, feat_names=[\"id\", \"d\"] + lag_features, merge_key=[\"id\", \"d\"], day=day)\n",
    "        \n",
    "    # ラベルエンコーディングする\n",
    "    le = LabelEncoder()\n",
    "    df[\"tm_y\"] = le.fit_transform(df[\"tm_y\"])\n",
    "    df[\"sell_price_state\"] = le.fit_transform(df[\"sell_price_state\"])\n",
    "    \n",
    "    # 特徴リストの生成(不要な特徴は除外する)\n",
    "    remove_features = ['id','state_id','store_id','date','wm_yr_wk','d', TARGET]+[\"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # START_TRAINの値を元に、前半のデータを取り除く\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    # 店舗IDごとの細かい期間指定\n",
    "    if store==\"WI_1\":\n",
    "        df = df[df[\"d\"]>=637].reset_index(drop=True)\n",
    "    if store==\"WI_3\":\n",
    "        df = df[df[\"d\"]>=455].reset_index(drop=True)\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORES_IDS = pd.read_csv(EVALUATION)['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRMSSE計算の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_df = pd.read_pickle(SW_DF)\n",
    "S = sw_df.s.values\n",
    "W = sw_df.w.values\n",
    "SW = sw_df.sw.values\n",
    "\n",
    "roll_mat_df = pd.read_pickle(ROLL_MAT)\n",
    "roll_index = roll_mat_df.index\n",
    "roll_mat_csr = csr_matrix(roll_mat_df.values)\n",
    "\n",
    "STORES_IDS = pd.read_csv(EVALUATION)['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "# ★店舗別のSWを取得\n",
    "SW_store = {}\n",
    "for i, store_id in enumerate(STORES_IDS):\n",
    "    SW_store[store_id] = SW[pd.Index(np.arange(42840))[roll_index.map(lambda x: (x[0] in [2, 7, 8, 11]) and store_id in x[1])]]\n",
    "\n",
    "# ★店舗別のroll_matを取得\n",
    "roll_mat_csr_store = {}\n",
    "for i, store_id in enumerate(STORES_IDS):\n",
    "    roll_mat_csr_store[store_id] = roll_mat_csr[pd.Index(np.arange(42840))[roll_index.map(lambda x: (x[0] in [2, 7, 8, 11]) and store_id in x[1])], i*3049:(i+1)*3049]\n",
    "del roll_mat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ★★店舗別にWRMSSEをレベル別で算出する関数を返す関数\n",
    "def get_wrmsse_per_level_func(store_id, sell_price, level=None):\n",
    "    def _wrmsse(preds, y_true, score_only=True, s = S, w = W, sw=SW_store):\n",
    "        # Function to do quick rollups:\n",
    "        def _rollup(v, store_id):\n",
    "            '''\n",
    "            v - np.array of size (30490 rows, n day columns)\n",
    "            v_rolledup - array of size (n, 42840)\n",
    "            '''\n",
    "            v = v.reshape(28, 3049).T\n",
    "            return roll_mat_csr_store[store_id]*v #(v.T*roll_mat_csr.T).T\n",
    "\n",
    "        '''\n",
    "        preds - Predictions: pd.DataFrame of size (30490 rows, N day columns)\n",
    "        y_true - True values: pd.DataFrame of size (30490 rows, N day columns)\n",
    "        sequence_length - np.array of size (42840,)\n",
    "        sales_weight - sales weights based on last 28 days: np.array (42840,)\n",
    "        '''\n",
    "        preds = preds / sell_price\n",
    "        y_true = y_true / sell_price\n",
    "        if score_only:\n",
    "            scores = np.sqrt(\n",
    "                        np.mean(\n",
    "                            np.square(_rollup(preds-y_true, store_id))\n",
    "                                ,axis=1)) * sw[store_id] #<-used to be mistake here\n",
    "            if level is None:\n",
    "                return np.sum(scores, axis=0)\n",
    "            \n",
    "            score_spans = [\n",
    "                (0, 1), (1, 4), (4, 11), (11, 3060)          \n",
    "            ]\n",
    "            score = np.sum(scores[score_spans[level][0]:score_spans[level][1]], axis=0)\n",
    "            return score\n",
    "        else: \n",
    "            score_matrix = (np.square(_rollup(preds-y_true)) * np.square(w)[:, None])/ s[:, None]\n",
    "            score = np.sum(np.sqrt(np.mean(score_matrix,axis=1)))/12 #<-used to be mistake here\n",
    "            return score, score_matrix\n",
    "\n",
    "    return _wrmsse\n",
    "\n",
    "def get_wrmsse_per_level_metric(store_id, sell_price, level=None):\n",
    "    wrmsse = get_wrmsse_per_level_func(store_id, sell_price, level=level)\n",
    "    def wrmsse_metric(preds, data):\n",
    "\n",
    "        # this function is calculate for last 28 days to consider the non-zero demand period\n",
    "\n",
    "        # actual obserbed values / 正解ラベル\n",
    "        y_true = data.get_label()\n",
    "        \n",
    "        if level is None:\n",
    "            return 'wrmsse-store', wrmsse(preds, y_true), False\n",
    "        \n",
    "        return 'wrmsse-level' + str(level), wrmsse(preds, y_true), False\n",
    "    return wrmsse_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollup(v):\n",
    "    '''\n",
    "    v - np.array of size (30490 rows, n day columns)\n",
    "    v_rolledup - array of size (n, 42840)\n",
    "    '''\n",
    "    return roll_mat_csr*v #(v.T*roll_mat_csr.T).T\n",
    "\n",
    "# Function to calculate WRMSSE:\n",
    "def wrmsse_metric(preds, y_true, score_only=False, s = S, w = W, sw=SW, level=None):\n",
    "    '''\n",
    "    preds - Predictions: pd.DataFrame of size (30490 rows, N day columns)\n",
    "    y_true - True values: pd.DataFrame of size (30490 rows, N day columns)\n",
    "    sequence_length - np.array of size (42840,)\n",
    "    sales_weight - sales weights based on last 28 days: np.array (42840,)\n",
    "    '''\n",
    "    \n",
    "    if score_only:\n",
    "        scores = np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(rollup(preds.values-y_true.values))\n",
    "                            ,axis=1)) * sw\n",
    "        if level is None:\n",
    "            return np.sum(scores)/12\n",
    "        \n",
    "        score_spans = [\n",
    "            (0, 1), (1, 4), (4, 14), (14, 17), (17, 24), (24, 33),\n",
    "            (33, 54), (54, 84), (84, 154), (154, 3203), (3203, 12350), (12350, 42840)           \n",
    "        ]\n",
    "        score = np.sum(scores[score_spans[level][0]:score_spans[level][1]], axis=0)/12\n",
    "        return score\n",
    "    else: \n",
    "        score_matrix = (np.square(rollup(preds.values-y_true.values)) * np.square(w)[:, None])/ s[:, None]\n",
    "        score = np.sum(np.sqrt(np.mean(score_matrix,axis=1)))/12 #<-used to be mistake here\n",
    "        return score, score_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITEMS = 30490\n",
    "DAYS_PRED = 28\n",
    "def wrmsse(preds, data):\n",
    "    \n",
    "    # this function is calculate for last 28 days to consider the non-zero demand period\n",
    "    \n",
    "    # actual obserbed values / 正解ラベル\n",
    "    y_true = data.get_label()\n",
    "    \n",
    "    y_true = y_true[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    preds = preds[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    # number of columns\n",
    "    num_col = DAYS_PRED\n",
    "    \n",
    "    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n",
    "    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n",
    "    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n",
    "    \n",
    "          \n",
    "    # train = weight_mat_csr*np.c_[reshaped_preds, reshaped_true]\n",
    "    train = roll_mat_csr*np.c_[reshaped_preds, reshaped_true]\n",
    "    \"\"\"\n",
    "    score = np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean( # ★ RMSSEの分子 1/h * SUM_t=n+1~n+h (Y_t - Y^_t)^2 に対応\n",
    "                        np.square(\n",
    "                            train[:,:num_col] - train[:,num_col:]) # ★ 予測誤差（Y_t - Y^_t）\n",
    "                        ,axis=1)\n",
    "                    \n",
    "                    / weight1) * weight2)\n",
    "    \"\"\"\n",
    "    score = np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(rollup(train[:,:num_col] - train[:,num_col:]))\n",
    "                            ,axis=1)) * sw)/12 #<-used to be mistake here\n",
    "    return 'wrmsse', score, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBMの学習(TS3-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params['seed'] = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORES_IDS = pd.read_csv(EVALUATION)['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "train before : 1942\n",
      "------------------------------------------------------\n",
      "Train TX_2 1日後予測モデル\n",
      "n_estimators 850\n",
      "[100]\tvalid_0's rmse: 7.21868\n",
      "[200]\tvalid_0's rmse: 6.78152\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result_list = []\n",
    "validation_score = []\n",
    "best_iteration_list = []\n",
    "\n",
    "for val_range in SPLIT_LIST:\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(\"train before : \" + str(val_range[0]))\n",
    "    print(\"------------------------------------------------------\")\n",
    "    \n",
    "    # Create Dummy DataFrame to store predictions\n",
    "    all_preds = pd.DataFrame() # 28日間のvalidation計算用\n",
    "    sto_preds = pd.DataFrame() # store毎のvalidation計算用\n",
    "\n",
    "    ######## store_id毎にモデルを学習する ########\n",
    "    for store_id in STORES_IDS:      \n",
    "        for day in range(1, 28+1):\n",
    "            print('Train', store_id, f'{day}日後予測モデル')\n",
    "            \n",
    "            # store_idを指定して、特徴を結合済みのデータフレームを取得する\n",
    "            grid_df, feature_columns = get_data_by_store(store_id, day=day)\n",
    "            \n",
    "            # CA_2については一部外れ値ぽいデータがあるので、validationに含まれなければ取り除く\n",
    "            if val_range[0]>1878 and store_id==\"CA_2\":\n",
    "                grid_df = grid_df[grid_df[\"d\"]!=1877]\n",
    "                grid_df = grid_df[grid_df[\"d\"]!=1878]\n",
    "                \n",
    "            # TARGETの変更\n",
    "            grid_df[TARGET] = grid_df[TARGET]*grid_df[\"sell_price\"]\n",
    "\n",
    "            train_mask = ((grid_df['d']>=START_TRAIN) & (grid_df['d']<=val_range[0]-1)) \n",
    "            valid_mask = ((grid_df['d']>val_range[0]-1-28) & (grid_df[\"d\"]<=val_range[1]))\n",
    "            valid_mask2 = ((grid_df['d']>val_range[0]-1-28) & (grid_df[\"d\"]<=val_range[1]-28))\n",
    "            \n",
    "            df_train = grid_df[train_mask][feature_columns]\n",
    "            df_valid = grid_df[valid_mask2][feature_columns]\n",
    "\n",
    "            lgb_params[\"n_estimators\"] = iteration_list[store_id][day-1]\n",
    "            print(\"n_estimators\", lgb_params[\"n_estimators\"])\n",
    "            \n",
    "            train_data = lgb.Dataset(df_train, \n",
    "                                     label=grid_df[train_mask][TARGET], \n",
    "                                     categorical_feature=categorical_feat)\n",
    "            valid_data = lgb.Dataset(df_valid,\n",
    "                                     label=grid_df[valid_mask2][TARGET],\n",
    "                                     categorical_feature=categorical_feat) \n",
    "\n",
    "            estimator = lgb.train(lgb_params,\n",
    "                                  train_data,\n",
    "                                  valid_sets = [valid_data],\n",
    "                                  verbose_eval = 100,\n",
    "                                  )\n",
    "\n",
    "            valid_result_df = grid_df[valid_mask][feature_columns + [\"d\", \"id\"]]\n",
    "            for_predict_df  = grid_df[valid_mask][feature_columns]\n",
    "            \n",
    "            # iterationを保存\n",
    "            best_iteration_list.append(estimator.best_iteration)\n",
    "\n",
    "            # この状態で保存\n",
    "            pred = estimator.predict(for_predict_df)\n",
    "            valid_result_df[\"pred\"] = pred\n",
    "            valid_result_df[[\"d\", \"id\", \"pred\"]].to_pickle(f\"./save3/save_{store_id}_day{day}_{val_range[0]}.pkl\")\n",
    "            \n",
    "            # TARGETの値を元に戻す\n",
    "            valid_result_df[\"pred\"] = valid_result_df[\"pred\"]/valid_result_df[\"sell_price\"]\n",
    "            \n",
    "            # {day}日後の予測結果をデータフレームに結合する\n",
    "            for PREDICT_DAY in range(day, day+1):\n",
    "                day_to_predict = val_range[0]-1+PREDICT_DAY\n",
    "                tmp = valid_result_df[valid_result_df[\"d\"]==day_to_predict].rename(columns={\"pred\":\"pred_\" + str(day_to_predict)})\n",
    "                tmp = tmp[[\"id\", \"pred_\" + str(day_to_predict)]]\n",
    "\n",
    "                if 'id' in list(sto_preds):\n",
    "                    sto_preds = sto_preds.merge(tmp, on=['id'], how='left')\n",
    "                else:\n",
    "                    sto_preds = tmp.copy()\n",
    "                    \n",
    "            sto_preds.to_pickle(f\"sto_preds_{store_id}_day{day}_{val_range[0]}.pkl\")\n",
    "    \n",
    "#         # ★予測結果を日付でソート\n",
    "#         cols = [\"pred_\" + str(x) for x in np.arange(val_range[0], val_range[1]+1)]\n",
    "#         sto_preds = sto_preds[[\"id\"] + cols]\n",
    "\n",
    "#         # Ground truth:\n",
    "#         sales =  pd.read_csv(EVALUATION)\n",
    "#         dayCols = [\"d_{}\".format(i) for i in range(val_range[0], val_range[1]+1)]\n",
    "#         y_true = sales[sales[\"store_id\"] == store_id][dayCols]\n",
    "\n",
    "#         valid_preds = sto_preds.drop(\"id\", axis=1)\n",
    "\n",
    "#         # ★店舗別にWRMSSEの値を算出\n",
    "#         store_score = store_wrmsse(valid_preds.values, y_true.values, score_only=True)\n",
    "\n",
    "#         print(f\"{store_id} wrmsse: {store_score}\")\n",
    "\n",
    "        # 店舗別の出力を集約していく\n",
    "        if 'id' in list(all_preds):\n",
    "            all_preds = pd.concat([all_preds, sto_preds])\n",
    "        else:\n",
    "            all_preds = sto_preds.copy()\n",
    "            \n",
    "        sto_preds = pd.DataFrame() # store毎の出力をリセット\n",
    "    \n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(\"---------------- calc CV score -----------------------\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n",
    "    # Predict\n",
    "    sales =  pd.read_csv(EVALUATION)\n",
    "    base_id_list = sales[\"id\"].to_list()\n",
    "    all_preds_val = pd.DataFrame({\"id\":base_id_list})\n",
    "    all_preds_val = pd.merge(all_preds_val, all_preds, on=\"id\", how=\"left\")\n",
    "    all_preds_val = all_preds_val.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.best_iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
